{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 419 - ASSIGNMENT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berkay Barış Turan - 28132"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial.distance import euclidean, cityblock, mahalanobis\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy.linalg import inv\n",
    "import mahotas as mh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory):\n",
    "    data, labels = [], []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.png'):\n",
    "            img = cv2.imread(os.path.join(directory, filename), cv2.IMREAD_GRAYSCALE)\n",
    "            label = filename.split('-')[0]\n",
    "            data.append(img)\n",
    "            labels.append(label)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data, train_labels = load_dataset('mpeg7shapeB/train')\n",
    "test_data, test_labels = load_dataset('mpeg7shapeB/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Distance Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_distance(u, v, VI):\n",
    "    return mahalanobis(u, v, VI)\n",
    "\n",
    "def extract_features(image):\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return np.zeros(3)  \n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    moments = cv2.moments(largest_contour)\n",
    "    area = moments['m00']\n",
    "    perimeter = cv2.arcLength(largest_contour, True)\n",
    "    aspect_ratio = float(image.shape[1]) / image.shape[0]\n",
    "    return np.array([area, perimeter, aspect_ratio])\n",
    "\n",
    "def extract_features_from_dataset(dataset):\n",
    "    features = []\n",
    "    for image in dataset:\n",
    "        feature = extract_features(image)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def nearest_neighbor_classify(test_sample, train_samples, train_labels, distance_func):\n",
    "    closest_label = None\n",
    "    min_distance = float('inf')\n",
    "    for train_sample, label in zip(train_samples, train_labels):\n",
    "        distance = distance_func(test_sample, train_sample)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_label = label\n",
    "    return closest_label\n",
    "\n",
    "def classify_and_evaluate(train_features, train_labels, test_features, test_labels, distance_func):\n",
    "    predicted_labels = []\n",
    "    for test_feature in test_features:\n",
    "        predicted_label = nearest_neighbor_classify(test_feature, train_features, train_labels, distance_func)\n",
    "        predicted_labels.append(predicted_label)\n",
    "    accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "    return accuracy\n",
    "\n",
    "def classify_and_evaluate2(train_features, train_labels, test_features, test_labels, distance_func, inv_cov_matrix=None):\n",
    "    predicted_labels = []\n",
    "    for test_feature in test_features:\n",
    "        if inv_cov_matrix is not None:\n",
    "            predicted_label = nearest_neighbor_classify(test_feature, train_features, train_labels, lambda u, v: distance_func(u, v, inv_cov_matrix))\n",
    "        else:\n",
    "            predicted_label = nearest_neighbor_classify(test_feature, train_features, train_labels, distance_func)\n",
    "        predicted_labels.append(predicted_label)\n",
    "    accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "    return accuracy\n",
    "\n",
    "def chi_squared_distance(vec1, vec2):\n",
    "    return np.sum((vec1 - vec2)**2 / (vec1 + vec2 + 1e-10))\n",
    "\n",
    "def fourier_descriptors(contour, num_coeffs):\n",
    "    contour = contour.squeeze()\n",
    "    min_length = num_coeffs * 2\n",
    "    if contour.shape[0] < min_length:\n",
    "        padding = np.zeros((min_length - contour.shape[0], 2))\n",
    "        contour = np.vstack((contour, padding))\n",
    "    contour_complex = contour[:, 0] + 1j * contour[:, 1]\n",
    "    fourier_result = np.fft.fft(contour_complex)\n",
    "    fourier_result = fourier_result / np.abs(fourier_result[0])\n",
    "    fd = np.abs(fourier_result[:num_coeffs])\n",
    "    return fd\n",
    "\n",
    "def extract_features2(image, num_coeffs):\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return np.zeros(num_coeffs)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    fd = fourier_descriptors(largest_contour, num_coeffs)\n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance: 0.5071428571428571\n",
      "Accuracy with Manhattan distance: 0.5171428571428571\n",
      "Accuracy with Mahalanobis distance: 0.5014285714285714\n",
      "Accuracy with Chi-squared distance: 0.5\n"
     ]
    }
   ],
   "source": [
    "train_features = extract_features_from_dataset(train_data)\n",
    "test_features = extract_features_from_dataset(test_data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "cov_matrix = np.cov(np.array(train_features_scaled).T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_norm = scaler.fit_transform(train_features)\n",
    "test_features_norm = scaler.transform(test_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, lambda u, v: mahalanobis_distance(u, v, inv_cov_matrix))\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_norm, train_labels, test_features_norm, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance: {accuracy_chi_squared}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Basic Shape Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area_descriptor(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        area = cv2.contourArea(largest_contour)\n",
    "        return area\n",
    "    else:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with area descriptor: 0.18714285714285714\n",
      "Accuracy with Manhattan distance with area descriptor: 0.18714285714285714\n",
      "Accuracy with Mahalanobis distance with area descriptor: 0.07285714285714286\n",
      "Accuracy with Chi-squared distance with area descriptor: 0.18714285714285714\n"
     ]
    }
   ],
   "source": [
    "def calculate_area_descriptor(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        area = cv2.contourArea(largest_contour)\n",
    "        return area\n",
    "    else:\n",
    "        return 0 \n",
    "        \n",
    "train_features = [calculate_area_descriptor(image) for image in train_data]\n",
    "test_features = [calculate_area_descriptor(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "expanded_train_features = np.hstack([train_features_scaled, train_features_scaled + np.random.randn(*train_features_scaled.shape) * 0.01])\n",
    "expanded_test_features = np.hstack([test_features_scaled, test_features_scaled + np.random.randn(*test_features_scaled.shape) * 0.01])\n",
    "\n",
    "cov_matrix = np.cov(expanded_train_features.T)\n",
    "regularized_cov_matrix = cov_matrix + 1e-5 * np.eye(cov_matrix.shape[0])\n",
    "inv_cov_matrix = inv(regularized_cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(expanded_train_features, train_labels, expanded_test_features, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with area descriptor: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with area descriptor: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with area descriptor: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with area descriptor: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Perimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perimeter_descriptor(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        perimeter = cv2.arcLength(largest_contour, True)\n",
    "        return perimeter\n",
    "    else:\n",
    "        return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with perimeter descriptor: 0.1457142857142857\n",
      "Accuracy with Manhattan distance with perimeter descriptor: 0.1457142857142857\n",
      "Accuracy with Mahalanobis distance with perimeter descriptor: 0.09857142857142857\n",
      "Accuracy with Chi-squared distance with perimeter descriptor: 0.1457142857142857\n"
     ]
    }
   ],
   "source": [
    "train_features = [calculate_perimeter_descriptor(image) for image in train_data]\n",
    "test_features = [calculate_perimeter_descriptor(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "expanded_train_features = np.hstack([train_features_scaled, train_features_scaled + np.random.randn(*train_features_scaled.shape) * 0.01])\n",
    "expanded_test_features = np.hstack([test_features_scaled, test_features_scaled + np.random.randn(*test_features_scaled.shape) * 0.01])\n",
    "\n",
    "cov_matrix = np.cov(expanded_train_features.T)\n",
    "regularized_cov_matrix = cov_matrix + 1e-5 * np.eye(cov_matrix.shape[0])\n",
    "inv_cov_matrix = inv(regularized_cov_matrix)\n",
    "\n",
    "accuracy_mahalanobis = classify_and_evaluate2(expanded_train_features, train_labels, expanded_test_features, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with perimeter descriptor: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with perimeter descriptor: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with perimeter descriptor: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with perimeter descriptor: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Convexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_convexity_descriptor(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        object_area = cv2.contourArea(largest_contour)\n",
    "        hull = cv2.convexHull(largest_contour)\n",
    "        hull_area = cv2.contourArea(hull)\n",
    "        if hull_area == 0:\n",
    "            return 0\n",
    "        convexity = object_area / hull_area\n",
    "        return convexity\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with convexity descriptor: 0.13\n",
      "Accuracy with Manhattan distance with convexity descriptor: 0.13\n",
      "Accuracy with Mahalanobis distance with convexity descriptor: 0.07285714285714286\n",
      "Accuracy with Chi-squared distance with convexity descriptor: 0.13\n"
     ]
    }
   ],
   "source": [
    "train_features = [calculate_convexity_descriptor(image) for image in train_data]\n",
    "test_features = [calculate_convexity_descriptor(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "expanded_train_features = np.hstack([train_features_scaled, train_features_scaled + np.random.randn(*train_features_scaled.shape) * 0.01])\n",
    "expanded_test_features = np.hstack([test_features_scaled, test_features_scaled + np.random.randn(*test_features_scaled.shape) * 0.01])\n",
    "\n",
    "cov_matrix = np.cov(expanded_train_features.T)\n",
    "regularized_cov_matrix = cov_matrix + 1e-5 * np.eye(cov_matrix.shape[0])\n",
    "inv_cov_matrix = inv(regularized_cov_matrix)\n",
    "\n",
    "accuracy_mahalanobis = classify_and_evaluate2(expanded_train_features, train_labels, expanded_test_features, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "print(f'Accuracy with Euclidean distance with convexity descriptor: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with convexity descriptor: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with convexity descriptor: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with convexity descriptor: {accuracy_chi_squared}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Circularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_circularity_descriptor(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        object_area = cv2.contourArea(largest_contour)\n",
    "        perimeter = cv2.arcLength(largest_contour, True)\n",
    "        if perimeter == 0:\n",
    "            return 0\n",
    "        circularity = (4 * np.pi * object_area) / (perimeter ** 2)\n",
    "        return circularity\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with circularity descriptor: 0.13142857142857142\n",
      "Accuracy with Manhattan distance with circularity descriptor: 0.13142857142857142\n",
      "Accuracy with Mahalanobis distance with circularity descriptor: 0.06285714285714286\n",
      "Accuracy with Chi-squared distance with circularity descriptor: 0.13142857142857142\n"
     ]
    }
   ],
   "source": [
    "train_features = [calculate_circularity_descriptor(image) for image in train_data]\n",
    "test_features = [calculate_circularity_descriptor(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "expanded_train_features = np.hstack([train_features_scaled, train_features_scaled + np.random.randn(*train_features_scaled.shape) * 0.01])\n",
    "expanded_test_features = np.hstack([test_features_scaled, test_features_scaled + np.random.randn(*test_features_scaled.shape) * 0.01])\n",
    "\n",
    "cov_matrix = np.cov(expanded_train_features.T)\n",
    "regularized_cov_matrix = cov_matrix + 1e-5 * np.eye(cov_matrix.shape[0])\n",
    "inv_cov_matrix = inv(regularized_cov_matrix)\n",
    "\n",
    "accuracy_mahalanobis = classify_and_evaluate2(expanded_train_features, train_labels, expanded_test_features, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "print(f'Accuracy with Euclidean distance with circularity descriptor: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with circularity descriptor: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with circularity descriptor: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with circularity descriptor: {accuracy_chi_squared}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Rectangularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rectangularity_descriptor(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        object_area = cv2.contourArea(largest_contour)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        bounding_rect_area = w * h\n",
    "\n",
    "        if bounding_rect_area == 0:\n",
    "            return 0\n",
    "\n",
    "        rectangularity = object_area / bounding_rect_area\n",
    "        return rectangularity\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with rectangularity descriptor: 0.11\n",
      "Accuracy with Manhattan distancewith rectangularity descriptor: 0.11\n",
      "Accuracy with Mahalanobis distance with rectangularity descriptor: 0.08142857142857143\n",
      "Accuracy with Chi-squared distance with rectangularity descriptor: 0.11\n"
     ]
    }
   ],
   "source": [
    "train_features = [calculate_rectangularity_descriptor(image) for image in train_data]\n",
    "test_features = [calculate_rectangularity_descriptor(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "expanded_train_features = np.hstack([train_features_scaled, train_features_scaled + np.random.randn(*train_features_scaled.shape) * 0.01])\n",
    "expanded_test_features = np.hstack([test_features_scaled, test_features_scaled + np.random.randn(*test_features_scaled.shape) * 0.01])\n",
    "\n",
    "cov_matrix = np.cov(expanded_train_features.T)\n",
    "regularized_cov_matrix = cov_matrix + 1e-5 * np.eye(cov_matrix.shape[0])\n",
    "inv_cov_matrix = inv(regularized_cov_matrix)\n",
    "\n",
    "accuracy_mahalanobis = classify_and_evaluate2(expanded_train_features, train_labels, expanded_test_features, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with rectangularity descriptor: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distancewith rectangularity descriptor: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with rectangularity descriptor: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with rectangularity descriptor: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f. Eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eccentricity_descriptor(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        if len(largest_contour) >= 5: \n",
    "            ellipse = cv2.fitEllipse(largest_contour)\n",
    "            (center, axes, orientation) = ellipse\n",
    "            major_axis_length = max(axes)\n",
    "            minor_axis_length = min(axes)\n",
    "            eccentricity = np.sqrt(1 - (minor_axis_length / major_axis_length) ** 2)\n",
    "            return eccentricity\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with eccentricity descriptor: 0.12571428571428572\n",
      "Accuracy with Manhattan distance with eccentricity descriptor: 0.12571428571428572\n",
      "Accuracy with Mahalanobis distance with eccentricity descriptor: 0.06714285714285714\n",
      "Accuracy with Chi-squared distance with eccentricity descriptor: 0.12571428571428572\n"
     ]
    }
   ],
   "source": [
    "train_features = [calculate_eccentricity_descriptor(image) for image in train_data]\n",
    "test_features = [calculate_eccentricity_descriptor(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "expanded_train_features = np.hstack([train_features_scaled, train_features_scaled + np.random.randn(*train_features_scaled.shape) * 0.01])\n",
    "expanded_test_features = np.hstack([test_features_scaled, test_features_scaled + np.random.randn(*test_features_scaled.shape) * 0.01])\n",
    "\n",
    "cov_matrix = np.cov(expanded_train_features.T)\n",
    "regularized_cov_matrix = cov_matrix + 1e-5 * np.eye(cov_matrix.shape[0])\n",
    "inv_cov_matrix = inv(regularized_cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(expanded_train_features, train_labels, expanded_test_features, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(np.array(train_features).reshape(-1, 1))\n",
    "test_features_scaled = scaler.transform(np.array(test_features).reshape(-1, 1))\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with eccentricity descriptor: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with eccentricity descriptor: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with eccentricity descriptor: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with eccentricity descriptor: {accuracy_chi_squared}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fourier Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num coefficient: 5\n",
      "Accuracy with Euclidean distance with fourier descriptor: 0.3628571428571429\n",
      "Accuracy with Manhattan distance with fourier descriptor: 0.36428571428571427\n",
      "Accuracy with Mahalanobis distance with fourier decriptor: 0.3142857142857143\n",
      "Accuracy with Chi-squared distance with fourier descriptor: 0.36142857142857143\n",
      "num coefficient: 10\n",
      "Accuracy with Euclidean distance with fourier descriptor: 0.5057142857142857\n",
      "Accuracy with Manhattan distance with fourier descriptor: 0.5\n",
      "Accuracy with Mahalanobis distance with fourier decriptor: 0.41\n",
      "Accuracy with Chi-squared distance with fourier descriptor: 0.4785714285714286\n",
      "num coefficient: 15\n",
      "Accuracy with Euclidean distance with fourier descriptor: 0.5057142857142857\n",
      "Accuracy with Manhattan distance with fourier descriptor: 0.4928571428571429\n",
      "Accuracy with Mahalanobis distance with fourier decriptor: 0.34\n",
      "Accuracy with Chi-squared distance with fourier descriptor: 0.4685714285714286\n",
      "num coefficient: 20\n",
      "Accuracy with Euclidean distance with fourier descriptor: 0.5057142857142857\n",
      "Accuracy with Manhattan distance with fourier descriptor: 0.4657142857142857\n",
      "Accuracy with Mahalanobis distance with fourier decriptor: 0.30857142857142855\n",
      "Accuracy with Chi-squared distance with fourier descriptor: 0.43142857142857144\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    num_coeffs = 5*i\n",
    "    print(f'num coefficient: {num_coeffs}')\n",
    "    \n",
    "    train_features = [extract_features2(image, num_coeffs) for image in train_data]\n",
    "    test_features = [extract_features2(image, num_coeffs) for image in test_data]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_features)\n",
    "    test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "    cov_matrix = np.cov(np.array(train_features_scaled).T)\n",
    "    inv_cov_matrix = inv(cov_matrix)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_features_norm = scaler.fit_transform(train_features)\n",
    "    test_features_norm = scaler.transform(test_features)\n",
    "\n",
    "    accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "    accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "    accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "    accuracy_chi_squared = classify_and_evaluate(train_features_norm, train_labels, test_features_norm, test_labels, chi_squared_distance)\n",
    "    \n",
    "    print(f'Accuracy with Euclidean distance with fourier descriptor: {accuracy_euclidean}')\n",
    "    print(f'Accuracy with Manhattan distance with fourier descriptor: {accuracy_manhattan}')\n",
    "    print(f'Accuracy with Mahalanobis distance with fourier decriptor: {accuracy_mahalanobis}')\n",
    "    print(f'Accuracy with Chi-squared distance with fourier descriptor: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Shape Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shape_histogram(contour, bins=10):\n",
    "    M = cv2.moments(contour)\n",
    "    cx = int(M['m10'] / M['m00'])\n",
    "    cy = int(M['m01'] / M['m00'])\n",
    "    centroid = np.array([cx, cy])\n",
    "\n",
    "    distances = np.sqrt(np.sum((contour - centroid) ** 2, axis=2))\n",
    "    histogram, _ = np.histogram(distances, bins=bins, range=[0, np.max(distances)])\n",
    "    histogram = histogram / np.sum(histogram) \n",
    "\n",
    "    return histogram\n",
    "\n",
    "def extract_features(image, bins=8):\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return np.zeros(bins)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    oh = calculate_shape_histogram(largest_contour, bins=bins)\n",
    "    return oh    \n",
    "\n",
    "def regularized_inverse(cov_matrix, alpha=1e-5):\n",
    "    return inv(cov_matrix + alpha * np.eye(cov_matrix.shape[0]))\n",
    "\n",
    "def classify_and_evaluate(train_features, train_labels, test_features, test_labels, distance_func, inv_cov_matrix=None):\n",
    "    predicted_labels = []\n",
    "    for test_feature in test_features:\n",
    "        if inv_cov_matrix is not None:\n",
    "            predicted_label = nearest_neighbor_classify(test_feature, train_features, train_labels, lambda u, v: distance_func(u, v, inv_cov_matrix))\n",
    "        else:\n",
    "            predicted_label = nearest_neighbor_classify(test_feature, train_features, train_labels, distance_func)\n",
    "        predicted_labels.append(predicted_label)\n",
    "    accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bins = 4\n",
      "Accuracy with Euclidean distance with shape histograms: 0.25\n",
      "Accuracy with Manhattan distance with shape histograms: 0.2542857142857143\n",
      "Accuracy with Mahalanobis distance with shape histograms: 0.23857142857142857\n",
      "Accuracy with Chi-squared distance with shape histograms: 0.24714285714285714\n",
      "bins = 8\n",
      "Accuracy with Euclidean distance with shape histograms: 0.4742857142857143\n",
      "Accuracy with Manhattan distance with shape histograms: 0.4785714285714286\n",
      "Accuracy with Mahalanobis distance with shape histograms: 0.4257142857142857\n",
      "Accuracy with Chi-squared distance with shape histograms: 0.48857142857142855\n",
      "bins = 12\n",
      "Accuracy with Euclidean distance with shape histograms: 0.53\n",
      "Accuracy with Manhattan distance with shape histograms: 0.56\n",
      "Accuracy with Mahalanobis distance with shape histograms: 0.46285714285714286\n",
      "Accuracy with Chi-squared distance with shape histograms: 0.5414285714285715\n",
      "bins = 16\n",
      "Accuracy with Euclidean distance with shape histograms: 0.49857142857142855\n",
      "Accuracy with Manhattan distance with shape histograms: 0.5271428571428571\n",
      "Accuracy with Mahalanobis distance with shape histograms: 0.4014285714285714\n",
      "Accuracy with Chi-squared distance with shape histograms: 0.52\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    bins = 4 * i\n",
    "    print(f'bins = {bins}')\n",
    "    train_features = [extract_features(image, bins) for image in train_data]\n",
    "    test_features = [extract_features(image, bins) for image in test_data]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_features)\n",
    "    test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "    accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "    accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "    cov_matrix = np.cov(np.array(train_features_scaled).T)\n",
    "    inv_cov_matrix = regularized_inverse(cov_matrix)\n",
    "    accuracy_mahalanobis = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    train_features_norm = scaler.fit_transform(train_features)\n",
    "    test_features_norm = scaler.transform(test_features)\n",
    "    accuracy_chi_squared = classify_and_evaluate(train_features_norm, train_labels, test_features_norm, test_labels, chi_squared_distance)\n",
    "\n",
    "    print(f'Accuracy with Euclidean distance with shape histograms: {accuracy_euclidean}')\n",
    "    print(f'Accuracy with Manhattan distance with shape histograms: {accuracy_manhattan}')\n",
    "    print(f'Accuracy with Mahalanobis distance with shape histograms: {accuracy_mahalanobis}')\n",
    "    print(f'Accuracy with Chi-squared distance with shape histograms: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Moment Invariants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Hu's Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with Hu moment invariants: 0.45714285714285713\n",
      "Accuracy with Manhattan distance with Hu moment invariants: 0.46\n",
      "Accuracy with Mahalanobis distance with Hu moment invariants: 0.4657142857142857\n",
      "Accuracy with Chi-squared distance with Hu moment invariants: 0.4542857142857143\n"
     ]
    }
   ],
   "source": [
    "def extract_hu_moments(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    moments = cv2.moments(image)\n",
    "    hu_moments = cv2.HuMoments(moments)\n",
    "    epsilon = 1e-10\n",
    "    hu_moments = -np.sign(hu_moments) * np.log10(np.abs(hu_moments) + epsilon)\n",
    "    return hu_moments.flatten()\n",
    "\n",
    "def extract_hu(image):\n",
    "    hu_features = extract_hu_moments(image)\n",
    "    return hu_features\n",
    "\n",
    "train_features = [extract_hu(image) for image in train_data]\n",
    "test_features = [extract_hu(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with Hu moment invariants: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with Hu moment invariants: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with Hu moment invariants: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with Hu moment invariants: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Zernike's Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with Zernike moment invariant: 0.12\n",
      "Accuracy with Manhattan distance with Zernike moment invariant: 0.11285714285714285\n",
      "Accuracy with Mahalanobis distance with Zernike moment invariant: 0.08142857142857143\n",
      "Accuracy with Chi-squared distance with Zernike moment invariant: 0.11571428571428571\n"
     ]
    }
   ],
   "source": [
    "def extract_zernike_moments(image, radius, degree=8):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "    zernike_moments = mh.features.zernike_moments(binary_image, radius, degree=degree)\n",
    "    return zernike_moments\n",
    "\n",
    "def extract_zernike(image, zernike_radius):\n",
    "    zernike_features = extract_zernike_moments(image, zernike_radius)\n",
    "    return zernike_features\n",
    "\n",
    "zernike_radius = 21\n",
    "\n",
    "train_features = [extract_zernike(image, zernike_radius) for image in train_data]\n",
    "test_features = [extract_zernike(image, zernike_radius) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with Zernike moment invariant: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with Zernike moment invariant: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with Zernike moment invariant: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with Zernike moment invariant: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Geometric Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with geometric moment invariant: 0.5242857142857142\n",
      "Accuracy with Manhattan distance with geometric moment invariant: 0.5285714285714286\n",
      "Accuracy with Mahalanobis distance with geometric moment invariant: 0.5471428571428572\n",
      "Accuracy with Chi-squared distance with geometric moment invariant: 0.52\n"
     ]
    }
   ],
   "source": [
    "def extract_geometric_moments(image):\n",
    "    moments = cv2.moments(image)\n",
    "    return np.array([moments['m00'], moments['m01'], moments['m10'], moments['m11']])\n",
    "\n",
    "def extract_geometric(image):\n",
    "    geometric_features = extract_geometric_moments(image)\n",
    "    return geometric_features\n",
    "\n",
    "train_features = [extract_geometric(image) for image in train_data]\n",
    "test_features = [extract_geometric(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with geometric moment invariant: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with geometric moment invariant: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with geometric moment invariant: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with geometric moment invariant: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Flusser Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with flusser moment invariant: 0.18857142857142858\n",
      "Accuracy with Manhattan distance with flusser moment invariant: 0.18857142857142858\n",
      "Accuracy with Chi-squared distance with flusser moment invariant: 0.18857142857142858\n"
     ]
    }
   ],
   "source": [
    "def extract_flusser_moments(image):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    moments = cv2.moments(image)\n",
    "    flusser_moments = []\n",
    "    if moments['mu02'] != 0:\n",
    "        flusser_moments.append(moments['mu20'] / moments['mu02'])\n",
    "    return np.array(flusser_moments)\n",
    "\n",
    "def extract_flusser(image):\n",
    "    flusser_features = extract_flusser_moments(image)\n",
    "    return flusser_features\n",
    "\n",
    "def classify_and_evaluate(train_features, train_labels, test_features, test_labels, distance_func, VI=None):\n",
    "    predicted_labels = []\n",
    "    for test_feature in test_features:\n",
    "        closest_label = None\n",
    "        min_distance = float('inf')\n",
    "        for train_feature, label in zip(train_features, train_labels):\n",
    "            if VI is not None:\n",
    "                distance = distance_func(test_feature, train_feature, VI)\n",
    "            else:\n",
    "                distance = distance_func(test_feature, train_feature)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_label = label\n",
    "        predicted_labels.append(closest_label)\n",
    "    return accuracy_score(test_labels, predicted_labels)\n",
    "\n",
    "train_features = [extract_flusser(image) for image in train_data]\n",
    "test_features = [extract_flusser(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\"\"\"\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\"\"\"\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with flusser moment invariant: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with flusser moment invariant: {accuracy_manhattan}')\n",
    "#print(f'Accuracy with Mahalanobis distance with flusser moment invariant: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with flusser moment invariant: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Combination 1: Area, Perimeter, and Circularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with combined features 1: 0.3314285714285714\n",
      "Accuracy with Manhattan distance with combined features 1: 0.33285714285714285\n",
      "Accuracy with Mahalanobis distance with combined features 1: 0.3242857142857143\n",
      "Accuracy with Chi-squared distance with combined features 1: 0.32142857142857145\n"
     ]
    }
   ],
   "source": [
    "def extract_combined_features(image):\n",
    "    area = calculate_area_descriptor(image)\n",
    "    perimeter = calculate_perimeter_descriptor(image)\n",
    "    circularity = calculate_circularity_descriptor(image)\n",
    "    combined_features = np.array([area, perimeter, circularity])\n",
    "    return combined_features\n",
    "\n",
    "train_combined_features = [extract_combined_features(image) for image in train_data]\n",
    "test_combined_features = [extract_combined_features(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_combined_features)\n",
    "test_features_scaled = scaler.transform(test_combined_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled_chi = scaler.fit_transform(train_combined_features)\n",
    "test_features_scaled_chi = scaler.transform(test_combined_features)\n",
    "accuracy_chi_squared = classify_and_evaluate2(train_features_scaled_chi, train_labels, test_features_scaled_chi, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with combined features 1: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with combined features 1: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with combined features 1: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with combined features 1: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Combination 2: Area, Perimeter, Circularity, Convexity, Rectangularity, and Eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with combined features 2: 0.6728571428571428\n",
      "Accuracy with Manhattan distance with combined features 2 0.6657142857142857\n",
      "Accuracy with Mahalanobis distance with combined features 2: 0.6657142857142857\n",
      "Accuracy with Chi-squared distance with combined features 2: 0.6442857142857142\n"
     ]
    }
   ],
   "source": [
    "def extract_all_combined_features(image):\n",
    "    area = calculate_area_descriptor(image)\n",
    "    perimeter = calculate_perimeter_descriptor(image)\n",
    "    convexity = calculate_convexity_descriptor(image)\n",
    "    circularity = calculate_circularity_descriptor(image)\n",
    "    rectangularity = calculate_rectangularity_descriptor(image)\n",
    "    eccentricity = calculate_eccentricity_descriptor(image)\n",
    "\n",
    "    combined_features = np.array([area, perimeter, convexity, circularity, rectangularity, eccentricity])\n",
    "    return combined_features\n",
    "\n",
    "train_combined_features = [extract_all_combined_features(image) for image in train_data]\n",
    "test_combined_features = [extract_all_combined_features(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_combined_features)\n",
    "test_features_scaled = scaler.transform(test_combined_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled_chi = scaler.fit_transform(train_combined_features)\n",
    "test_features_scaled_chi = scaler.transform(test_combined_features)\n",
    "accuracy_chi_squared = classify_and_evaluate2(train_features_scaled_chi, train_labels, test_features_scaled_chi, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with combined features 2: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with combined features 2 {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with combined features 2: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with combined features 2: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Combination 3: Area, Perimeter, Circularity, Convexity, Rectangularity, and Eccentricity with Fourier Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with combined features 3: 0.7471428571428571\n",
      "Accuracy with Manhattan distance with combined features 3: 0.78\n",
      "Accuracy with Mahalanobis distance with combined features 3: 0.6885714285714286\n",
      "Accuracy with Chi-squared distance with combined features 3: 0.7328571428571429\n"
     ]
    }
   ],
   "source": [
    "def extract_all_combined_features(image, num_coeffs=10):\n",
    "    area = calculate_area_descriptor(image)\n",
    "    perimeter = calculate_perimeter_descriptor(image)\n",
    "    convexity = calculate_convexity_descriptor(image)\n",
    "    circularity = calculate_circularity_descriptor(image)\n",
    "    rectangularity = calculate_rectangularity_descriptor(image)\n",
    "    eccentricity = calculate_eccentricity_descriptor(image)\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        fourier_features = fourier_descriptors(largest_contour, num_coeffs)\n",
    "    else:\n",
    "        fourier_features = np.zeros(num_coeffs)\n",
    "    combined_features = np.concatenate(([area, perimeter, convexity, circularity, rectangularity, eccentricity], fourier_features))\n",
    "    return combined_features\n",
    "\n",
    "train_combined_features = [extract_all_combined_features(image, num_coeffs=10) for image in train_data]\n",
    "test_combined_features = [extract_all_combined_features(image, num_coeffs=10) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_combined_features)\n",
    "test_features_scaled = scaler.transform(test_combined_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled_chi = scaler.fit_transform(train_combined_features)\n",
    "test_features_scaled_chi = scaler.transform(test_combined_features)\n",
    "accuracy_chi_squared = classify_and_evaluate2(train_features_scaled_chi, train_labels, test_features_scaled_chi, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with combined features 3: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with combined features 3: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with combined features 3: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with combined features 3: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Combination 4: Area, Perimeter, Circularity, Convexity, Rectangularity and Eccentricity with Shape Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with with combined features 4: 0.7142857142857143\n",
      "Accuracy with Manhattan distance with with combined features 4: 0.7742857142857142\n",
      "Accuracy with Mahalanobis distance with with combined features 4: 0.7071428571428572\n",
      "Accuracy with Chi-squared distance with with combined features 4: 0.7528571428571429\n"
     ]
    }
   ],
   "source": [
    "def extract_all_combined_features(image, bins=12):\n",
    "    area = calculate_area_descriptor(image)\n",
    "    perimeter = calculate_perimeter_descriptor(image)\n",
    "    convexity = calculate_convexity_descriptor(image)\n",
    "    circularity = calculate_circularity_descriptor(image)\n",
    "    rectangularity = calculate_rectangularity_descriptor(image)\n",
    "    eccentricity = calculate_eccentricity_descriptor(image)\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        shape_histogram = calculate_shape_histogram(largest_contour, bins)\n",
    "    else:\n",
    "        shape_histogram = np.zeros(bins)\n",
    "    combined_features = np.concatenate(([area, perimeter, convexity, circularity, rectangularity, eccentricity], shape_histogram))\n",
    "    return combined_features\n",
    "\n",
    "train_combined_features = [extract_all_combined_features(image, bins=12) for image in train_data]\n",
    "test_combined_features = [extract_all_combined_features(image, bins=12) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_combined_features)\n",
    "test_features_scaled = scaler.transform(test_combined_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled_chi = scaler.fit_transform(train_combined_features)\n",
    "test_features_scaled_chi = scaler.transform(test_combined_features)\n",
    "accuracy_chi_squared = classify_and_evaluate2(train_features_scaled_chi, train_labels, test_features_scaled_chi, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with with combined features 4: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with with combined features 4: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with with combined features 4: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with with combined features 4: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Combination 5: Area, Perimeter, Circularity, Convexity, Rectangularity and Eccentricity with Shape Histogram and Fourier Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\berka\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\spatial\\distance.py:1023: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(m)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with with combined features 5: 0.7971428571428572\n",
      "Accuracy with Manhattan distance with with combined features 5: 0.8314285714285714\n",
      "Accuracy with Mahalanobis distance with with combined features 5: 0.6585714285714286\n",
      "Accuracy with Chi-squared distance with with combined features 5: 0.8128571428571428\n"
     ]
    }
   ],
   "source": [
    "def extract_all_combined_features(image, bins=12, num_coeffs=10):\n",
    "    area = calculate_area_descriptor(image)\n",
    "    perimeter = calculate_perimeter_descriptor(image)\n",
    "    convexity = calculate_convexity_descriptor(image)\n",
    "    circularity = calculate_circularity_descriptor(image)\n",
    "    rectangularity = calculate_rectangularity_descriptor(image)\n",
    "    eccentricity = calculate_eccentricity_descriptor(image)\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        shape_histogram = calculate_shape_histogram(largest_contour, bins)\n",
    "        fourier_features = fourier_descriptors(largest_contour, num_coeffs)\n",
    "    else:\n",
    "        shape_histogram = np.zeros(bins)\n",
    "        fourier_features = np.zeros(num_coeffs)\n",
    "    combined_features = np.concatenate(([area, perimeter, convexity, circularity, rectangularity, eccentricity], shape_histogram, fourier_features))\n",
    "    return combined_features\n",
    "\n",
    "train_combined_features = [extract_all_combined_features(image, bins=12, num_coeffs=10) for image in train_data]\n",
    "test_combined_features = [extract_all_combined_features(image, bins=12, num_coeffs=10) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_combined_features)\n",
    "test_features_scaled = scaler.transform(test_combined_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled_chi = scaler.fit_transform(train_combined_features)\n",
    "test_features_scaled_chi = scaler.transform(test_combined_features)\n",
    "accuracy_chi_squared = classify_and_evaluate2(train_features_scaled_chi, train_labels, test_features_scaled_chi, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with with combined features 5: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with with combined features 5: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with with combined features 5: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with with combined features 5: {accuracy_chi_squared}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f. Combination 6: Hu's Moment and Geometric Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with combined features 6: 0.5357142857142857\n",
      "Accuracy with Manhattan distance with combined features 6: 0.57\n",
      "Accuracy with Mahalanobis distance with combined features 6: 0.5642857142857143\n",
      "Accuracy with Chi-squared distance with combined features 6: 0.5557142857142857\n"
     ]
    }
   ],
   "source": [
    "def extract_combined_features_hu_geo(image):\n",
    "    hu_features = extract_hu_moments(image)\n",
    "    geometric_features = extract_geometric_moments(image)\n",
    "    combined_features = np.hstack([\n",
    "        hu_features,\n",
    "        geometric_features,\n",
    "    ])\n",
    "    return combined_features\n",
    "\n",
    "train_features = [extract_combined_features_hu_geo(image) for image in train_data]\n",
    "test_features = [extract_combined_features_hu_geo(image) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with combined features 6: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with combined features 6: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with combined features 6: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with combined features 6: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g. Combination 7: Hu's, Geometric, Zernike, and Flusser Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with combined features 7: 0.5157142857142857\n",
      "Accuracy with Manhattan distance with combined features 7: 0.5242857142857142\n",
      "Accuracy with Mahalanobis distance with combined features 7: 0.45285714285714285\n",
      "Accuracy with Chi-squared distance with combined features 7: 0.4685714285714286\n"
     ]
    }
   ],
   "source": [
    "def extract_combined_features2(image, zernike_radius):\n",
    "    hu_features = extract_hu_moments(image)\n",
    "    zernike_features = extract_zernike_moments(image, zernike_radius)\n",
    "    geometric_features = extract_geometric_moments(image)\n",
    "    flusser_features = extract_flusser_moments(image)\n",
    "    combined_features = np.hstack([\n",
    "        hu_features,\n",
    "        zernike_features,\n",
    "        geometric_features,\n",
    "        flusser_features\n",
    "    ])\n",
    "    return combined_features\n",
    "\n",
    "zernike_radius = 21\n",
    "train_features = [extract_combined_features2(image, zernike_radius) for image in train_data]\n",
    "test_features = [extract_combined_features2(image, zernike_radius) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with combined features 7: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with combined features 7: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with combined features 7: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with combined features 7: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h. Combination 8: Area, Perimeter, Circularity, Convexity, Rectangularity, Eccentricity, Shape Histogram, Fourier Descriptor, Hu's Moment, and Geometric Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\berka\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\spatial\\distance.py:1023: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(m)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Euclidean distance with combined features 8: 0.7857142857142857\n",
      "Accuracy with Manhattan distance with combined features 8: 0.8514285714285714\n",
      "Accuracy with Mahalanobis distance with combined features 8: 0.6942857142857143\n",
      "Accuracy with Chi-squared distance with combined features 8: 0.7028571428571428\n"
     ]
    }
   ],
   "source": [
    "def extract_all_features(image, bins, num_coeffs):\n",
    "    combined_features1 = extract_all_combined_features(image, bins=bins, num_coeffs=num_coeffs)\n",
    "    combined_features2 = extract_combined_features_hu_geo(image)\n",
    "    all_combined_features = np.concatenate((combined_features1, combined_features2))\n",
    "    return all_combined_features\n",
    "\n",
    "train_all_combined_features = [extract_all_features(image, bins=12, num_coeffs=10) for image in train_data]\n",
    "test_all_combined_features = [extract_all_features(image, bins=12, num_coeffs=10) for image in test_data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_all_combined_features)\n",
    "test_features_scaled = scaler.transform(test_all_combined_features)\n",
    "\n",
    "accuracy_euclidean = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, euclidean)\n",
    "accuracy_manhattan = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, cityblock)\n",
    "\n",
    "cov_matrix = np.cov(train_features_scaled.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "accuracy_mahalanobis = classify_and_evaluate2(train_features_scaled, train_labels, test_features_scaled, test_labels, mahalanobis_distance, inv_cov_matrix)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_all_combined_features)\n",
    "test_features_scaled = scaler.transform(test_all_combined_features)\n",
    "accuracy_chi_squared = classify_and_evaluate(train_features_scaled, train_labels, test_features_scaled, test_labels, chi_squared_distance)\n",
    "\n",
    "print(f'Accuracy with Euclidean distance with combined features 8: {accuracy_euclidean}')\n",
    "print(f'Accuracy with Manhattan distance with combined features 8: {accuracy_manhattan}')\n",
    "print(f'Accuracy with Mahalanobis distance with combined features 8: {accuracy_mahalanobis}')\n",
    "print(f'Accuracy with Chi-squared distance with combined features 8: {accuracy_chi_squared}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
